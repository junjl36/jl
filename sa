<!DOCTYPE html>
<html lang="en">
<head>
<title>myweb</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Poppins">
<style>
body,h1,h2,h3,h4,h5 {font-family: "Poppins", sans-serif}
body {font-size:16px;}
.w3-half img{margin-bottom:-6px;margin-top:16px;opacity:0.8;cursor:pointer}
.w3-half img:hover{opacity:1}
</style>
</head>
<body>

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-red w3-collapse w3-top w3-large w3-padding" style="z-index:3;width:300px;font-weight:bold;" id="mySidebar"><br>
  <a href="javascript:void(0)" onclick="w3_close()" class="w3-button w3-hide-large w3-display-topleft" style="width:100%;font-size:22px">Close Menu</a>
  <div class="w3-container">
    <h3 class="w3-padding-64"><b>CS1102<br>Course Project - 2023/2024 Semester B</b></h3>
  <p>Li Tik Bond Douglas 56622926</p>
    <p>Chan Wing Chung 56616540</p>
    <p>Cheung Sze Wan 57159900</p>
    <p>Wong Kai Hei 57151949</p>
  </div>
  <div class="w3-bar-block">
    <a href="#" onclick="w3_close()" class="w3-bar-item w3-button w3-hover-white">Home</a> 
    <a href="#Introduction" onclick="w3_close()" class="w3-bar-item w3-button w3-hover-white">Introduction</a> 
    <a href="#Pros" onclick="w3_close()" class="w3-bar-item w3-button w3-hover-white">Pros</a> 
    <a href="#Cons" onclick="w3_close()" class="w3-bar-item w3-button w3-hover-white">Cons</a> 
    <a href="#Limitation" onclick="w3_close()" class="w3-bar-item w3-button w3-hover-white">Limitation</a> 
    <a href="#Referencelist" onclick="w3_close()" class="w3-bar-item w3-button w3-hover-white">Reference Lists</a> 
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-container w3-top w3-hide-large w3-red w3-xlarge w3-padding">
  <a href="javascript:void(0)" class="w3-button w3-red w3-margin-right" onclick="w3_open()">☰</a>
  <span>CS1102</span>
</header>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:340px;margin-right:40px">

  <!-- Header -->
  <div class="w3-container" style="margin-top:80px" id="Introduction">
    <h1 class="w3-jumbo"><b>The pros and cons of Large Language Models (LLMs)</b></h1>
    <h1 class="w3-xxxlarge w3-text-red"><b>Introduction-History & Architecture</b></h1>
    <hr style="width:50px;border:5px solid red" class="w3-round">
  </div>
<h2>History</h2>
     <p>
      In 1950, the foundation of LLM was used for doing the experiments with neural networks and neural information processing systems, which allowed it to deal with natural language. That foundation is used to auto translate from Russian to English.
    </p>
    <p>
      In 1960, the first idea of LLM, Eliza, was created by MIT researcher Joseph Weizenbaum. At that time, Eliza was the first chatbot in the world. The creation of it has increased people’s attention in the area of natural language processing and LLM.
    </p>
    <p>
      In 1997, Long short-term memory (LSTM) network was created. The difference between LSTM and the model before this is that LSTM can address the vanishing gradient problem. This allows LSTM to capture and preserve important dependencies because it can selectively remember or forget information over long sequences.
    </p>
    <p>
      IN 2018, BERT (Bidirectional Encoder Representations from Transformers) was invented, which model uses the revolutionary structure called Transformer. The Transformer structure can predict the following context of each word and do tokenization simultaneously. This approach allows BERT to have a deeper understanding of the context and the meaning of the words.
    </p>
    <p>
      In 2020, this is the era of GPT (Generative Pre-trained Transformer) which was released by OpenAI. GPT is one of the most powerful LLM. For GPT-1, it only has 117 millions parameters for the model. For GPT-2, it has 1.5 billion parameters. For GPT-3, the number of parameters has increased to 175 billion. For the recent one GPT-5, it has 1.76 trillion parameters. The huge amount of parameters allow GPT to have strong ability for perform a wide range of language tasks, including text generation, translation, summarization, and question answering.
    </p>
<h2>Architecture</h2>
    <p>
     To understand LLM, it is necessary to understand the structure of Transformer. Before Transformers is invented, the main structure of LLM is Recurrent neural network (RNN), RNN deal with the input according to the sequence of the input, output of every step depends on the the hidden status of before and input of now, and the new calculation needs to wait for the finish of last step. It causes the efficiency of training to be relatively low. Move rover, RNN is not good at dealing with long sequences of input and long context. Due to the structure of RNN, the longer the input is, the smaller the front context affects the context at the back. Therefore, it is hard for RNN to capture the relationship between words in a long sentence. But in natural language, it is normal to have information depending on a long distance. Although there is LSTM to address this kind of long sequences problem, it still faces the problem of inability to concurrent computing.
    </p>
    <p>
      To solve the problem mentioned above, a transfer structure has been created. It has the ability of learning dependencies of all input sequences. It will not be affected by the short memory. The attention mechanism is the reason why transformers can have those features. While dealing with the word from the input in transformers, it not only concerns the word itself, but transformers will also concern all the other words in the input sequences and give every word weight of attention. Weight of attention is learned while the model is training step by step. Therefore, transformers can know the dependencies between the word dealing with and other words in the input, and thus focus on the important part of the context. Although the distance between two words is far, transformers can still capture the dependencies between two words.
    </p>
    <p>
     Another feature of the transformer is positional encoding, which can help the model to process the input sequences in parallel, meaning that it does not need to consider the order of input sequences. In Transformer, every word in the input sequences will be embedded into the word vector, and the positional vector before entering them into the neural network. This transformation can help the model understand the meaning of each word and capture the position of each word in the sentence. Word can input different sequences while using a transformer. Model can process all the positions of the input sequences at the same time. It doesn’t need to process according to the order of the input as RNN does. Every input of the Transformer model can do the calculation separately, and don’t need to wait for the result of another word. It increases the efficiency of model training. Therefore, huge models are relatively easy to train using transformers. 
    </p>	

  <!-- Pros -->
  <div class="w3-container" id="Pros" style="margin-top:75px">
    <h1 class="w3-xxxlarge w3-text-red"><b>Pros</b></h1>
    <hr style="width:50px;border:5px solid red" class="w3-round">
    <h2>Help in content creation</h2>
    <p>
     LLM collected vast amounts of data from basically everywhere. Recently, so many tools have helped people to create various content in a much faster way than ever before. Including stories, summarization, subtitles, or even academic essays. In the past, when someone needed to write paragraphs about a specific topic, they needed to use a lot of time to find references and process that information to write their words and understand the topics. 
    </p>
    <p>
     However, after the popularization of LLM, everything changed. People can enter keywords of the topics, and LLM will give you the information you need almost instantly. Moreover, the information is already summarized. Therefore, when creating content, LLM can provide us with an integrated outline and information about the topics, which can help people brainstorm and offer an example to refer to in a short period of time. For example, after we enter the keywords, LLM can generate a large number of viewpoints and perspectives on an essay topic for brainstorming or extending the content of a story automatically for us when we lack inspiration.
    </p>
    <h2>Knowledge learning and exploring</h2>
    <p>
     LLM is like an upgraded version of the encyclopedia. It knows everything we know, and everything we do not know. In terms of the quantity of knowledge, humans can not even be qualified to compete with LLM and it is not even close. In daily life, when people are confused about a topic or a weird question in their mind, they usually cannot find a person who knows the answer. When they are searching for the answer on the Internet, the answer may not be integrated and it may be too lengthy or complicated for a person without knowledge about that aspect. This might have stopped and blocked the imagination and curiosity toward knowledge of people so many times in the past. 
    </p>
    <p>
     For now, with the help of LLM, we can ask the questions we are curious about to LLM anywhere at any time. LLM can provide us with integrated answers. For those who have further questions, they can ask instantly and LLM will keep answering based on the previous questions and information. It is very convenient for people to learn something deeply and familiar with a topic. Before the invention of LLM, people had to ask a real person with the relevant knowledge to obtain a similar effect, which was difficult to find and would waste others’ time. In my personal experience, when I was having problems understanding the lecturer and the slides in some lessons, I could simply ask the point I got confused about the topics further and further to LLM until I fully understood the concept, which is much more efficient than searching the answers by ourselves or finding someone who is capable to teach us.
    </p>
    <section>
    <h2>Lower the threshold of professional skills</h2>
    <p>
     Coding is usually known as a very professional and difficult skill in public cognition. In fact, most people do not have knowledge about coding or other basic knowledge of various professional aspects. For the public who try to solve a question above their education level or out of their knowledge area. It may take tons of time to learn about the basic concepts and principles first for them to even have a sense of what are the questions asking for. But with the help of LLM, people can simply throw the problems or questions to the LLM, and LLM can provide them with a framework or process on how to solve the problem. Usually, a person without related knowledge cannot even understand the questions. However, with the help of LLM, those people can have access and a chance to finish the questions. Although the answer may not be correct, it provides an opportunity for people to take a big step on the question instead of learning from the very beginning. By combining the pros of knowledge exploration previously mentioned, they might even be able to solve some easy problems.
    </p>	
  </div>
  
  <!-- Cons -->
  <div class="w3-container" id="Cons" style="margin-top:75px">
    <h1 class="w3-xxxlarge w3-text-red"><b>Cons</b></h1>
    <hr style="width:50px;border:5px solid red" class="w3-round">
    
  </div>


  <!-- Limitation -->
<div class="w3-container" id="Limitation" style="margin-top:75px">
    <h1 class="w3-xxxlarge w3-text-red"><b>Limitation</b></h1>
    <hr style="width:50px;border:5px solid red" class="w3-round">
	</div>

 <div class="w3-container" id="Referencelist" style="margin-top:75px">
<h1 class="w3-xxxlarge w3-text-red"><b>Reference Lists</b></h1>
    <hr style="width:50px;border:5px solid red" class="w3-round">
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., and Polosukhin, I. (2017). "Attention Is All You Need." In Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. Retrieved from: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</p>
<p>Toloka Team. (2023, June 26). The history, timeline, and future of LLMs. Retrieved from https://toloka.ai/blog/history-of-llms/</p>
<p>
Kasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Günnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., . . . Kasneci, G. (2023). ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 103, 102274. https://doi.org/10.1016/j.lindif.2023.102274</p>
<p>Abdelghani, R., Wang, Y., Yuan, X., Wang, T., Lucas, P., Sauzéon, H., & Oudeyer, P. (2023c). GPT-3-Driven Pedagogical Agents to train children’s Curious Question-Asking skills. International Journal of Artificial Intelligence in Education. https://doi.org/10.1007/s40593-023-00340-7
</p>
<p>Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., Sutton, C. (2021). Program Synthesis with Large Language Models. arXiv. https://arxiv.org/pdf/2108.07732.pdf</p>
<p>Ippolito, Daphne. (2023). Understanding the Limitations of Using Large Language Models for Text Generation. Thesis (Ph.D.)--University of Pennsylvania, 2023.
https://pqdd-sinica-edu-tw.ezproxy.cityu.edu.hk/doc/30312969
</p>
<p>Jakesch, Maurice. (2022). Assessing the Effects and Risks of Large Language Models in AI-Mediated Communication. Thesis (Ph.D.)--Cornell University, 2022.
https://pqdd-sinica-edu-tw.ezproxy.cityu.edu.hk/doc/29400004
</p>
<p>Levy, S. Gabriel. (2023). Responsible AI via Responsible Large Language Models. Thesis (Ph.D.)--University of California, Santa Barbara, 2023.
file:///Users/szewancheung/Downloads/[30529918]%20Responsible_AI_via_Responsible_Large_Language_Models.pdf
</p>
</div>
<!-- End page content -->
</div>

<!-- W3.CSS Container -->
<div class="w3-light-grey w3-container w3-padding-32" style="margin-top:75px;padding-right:58px"><p class="w3-right"> 

<script>
// Script to open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}

// Modal Image Gallery
function onClick(element) {
  document.getElementById("img01").src = element.src;
  document.getElementById("modal01").style.display = "block";
  var captionText = document.getElementById("caption");
  captionText.innerHTML = element.alt;
}
</script>

</body>
</html>
